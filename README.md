# Pewlett Hackard Analysis

## Converting data from CSV to SQL Database
  The initial data is spread across 6 CSV files. The following ERD was created to visaulize the conections, and data types within the CSV files.

![EmployeeDB.png](https://github.com/jburs/Pewlett_Hackard_Analysis/blob/master/EmployeeDB.png)

  Using the ERD, a series of SQL queeries similar to the following were used to import the CSV data into SQL, and establish Keys and relationships. The full queerie is viewable in this repository as the schema. 

CREATE TABLE employees (
	emp_no INT NOT NULL,
	birth_date DATE NOT NULL,
	first_name VARCHAR NOT NULL,
	late_name VARCHAR NOT NULL,
	gender VARCHAR NOT NULL,
	hire_date DATE NOT NULL,
	PRIMARY KEY(emp_no)
);

## Table 1: Number of Retiring Employees by Title
  The number of retiring employees by title was found by joining the employee number, first and last name, title, from date, and salary. These values were taken from the retirement_info, titles, and salaries data frames. Then joined together using the common key employee number. The retirement info data frame was generated from the employees data frame, where only employees that are elible to retire were selected. The number of retiring employees data frame was then partitioned, to remove duplicates created when eployees changed positions within the company, leaving only their current position. This final data frame is called 'retire_by_title'. It contains 41,380 employees that are retiring, meaning 41,380 employees will need to be hired to replace them. 

  Difficulties were encountered when constucting the code to eliminate duplicates columns. These were solved through reviewing references and further reading. 

The following query was used to generate the data. 

-- Number of Retiring Employees by Title
SELECT ri.emp_no,
	ri.first_name,
	ri.last_name,
	ti.title,
	ti.from_date,
	s.salary
INTO silver_tsunami
FROM retirement_info as ri
INNER JOIN titles as ti
ON (ri.emp_no = ti.emp_no)
INNER JOIN salaries as s
ON (ti.emp_no = s.emp_no)
ORDER BY emp_no ASC;
	

select * FROM silver_tsunami;

-- Partition the data to show only most recent title per employee
SELECT emp_no,
	first_name, 
	last_name, 
	title, 
	from_date,
	salary
--INTO retire_by_title
FROM 
(SELECT emp_no,
	first_name, 
	last_name, 
	title, 
	from_date,
	salary, ROW_NUMBER() OVER
	(PARTITION BY (emp_no)
	ORDER BY from_date DESC) rn
	FROM silver_tsunami
) tmp WHERE rn = 1
ORDER BY emp_no;

select * from retire_by_title;

select count(retire_by_title.emp_no) FROM retire_by_title;


## Table 2: Mentorship Eligibility
The mentorship elibility table, 'mentor_eligible.csv', contains data on employees born between January 1, 1965 and December 31, 1965. This table contains informatino of 1,940 employees that are eligilbe to become mentors. This data was generated by joining columns from the empoyees data set, and titles data set. Duplicates were then removed and the CSV was saved. 

SELECT e.emp_no,
	e.first_name,
	e.last_name,
	e.birth_date,
	ti.title,
	ti.from_date,
	ti.to_date
into mentor_eligible_rough
FROM employees as e
INNER JOIN titles as ti
ON (e.emp_no = ti.emp_no)
WHERE (e.birth_date BETWEEN '1965-01-01' AND '1965-12-31')
ORDER BY emp_no ASC;

-- Partition the data to show only most recent title per employee
SELECT emp_no,
	birth_date,
	first_name, 
	last_name, 
	title, 
	from_date,
	to_date
INTO mentor_eligible
FROM 
(SELECT emp_no,
 	birth_date,
	first_name, 
	last_name, 
	title, 
	from_date,
	to_date, ROW_NUMBER() OVER
	(PARTITION BY (emp_no)
	ORDER BY from_date DESC) rn
	FROM mentor_eligible_rough
) tmp WHERE rn = 1
ORDER BY emp_no;
